{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<br />\n",
    "<div id=\"toc\"><ul class=\"toc\"><li><a href=\"#1.-Read-in-data\">1. Read in data</a><a class=\"anchor-link\" href=\"#1.-Read-in-data\">¶</a></li><li><a href=\"#2.-Clean-data\">2. Clean data</a><a class=\"anchor-link\" href=\"#2.-Clean-data\">¶</a></li><ul class=\"toc\"><li><a href=\"#2.1.-Check-for-missing-values\">2.1. Check for missing values</a><a class=\"anchor-link\" href=\"#2.1.-Check-for-missing-values\">¶</a></li><li><a href=\"#2.2.-Remove-uninformative-features\">2.2. Remove uninformative features</a><a class=\"anchor-link\" href=\"#2.2.-Remove-uninformative-features\">¶</a></li><li><a href=\"#2.3.-Remove-features-leaking-info-on-target\">2.3. Remove features leaking info on target</a><a class=\"anchor-link\" href=\"#2.3.-Remove-features-leaking-info-on-target\">¶</a></li></ul><li><a href=\"#3.-Process-features\">3. Process features</a><a class=\"anchor-link\" href=\"#3.-Process-features\">¶</a></li><ul class=\"toc\"><li><a href=\"#3.1.-Min-max-scale-continuous-variables\">3.1. Min-max scale continuous variables</a><a class=\"anchor-link\" href=\"#3.1.-Min-max-scale-continuous-variables\">¶</a></li><li><a href=\"#3.2.-One-hot-encode-nominal-data\">3.2. One-hot encode nominal data</a><a class=\"anchor-link\" href=\"#3.2.-One-hot-encode-nominal-data\">¶</a></li></ul><li><a href=\"#4.-Predict-and-evaluate\">4. Predict and evaluate</a><a class=\"anchor-link\" href=\"#4.-Predict-and-evaluate\">¶</a></li><li><a href=\"#5.-Closing-remarks\">5. Closing remarks</a><a class=\"anchor-link\" href=\"#5.-Closing-remarks\">¶</a></li><ul class=\"toc\"><li><a href=\"#5.1.-RMSE-instead-of-MAE\">5.1. RMSE instead of MAE</a><a class=\"anchor-link\" href=\"#5.1.-RMSE-instead-of-MAE\">¶</a></li><li><a href=\"#5.2.-Correlated-features\">5.2. Correlated features</a><a class=\"anchor-link\" href=\"#5.2.-Correlated-features\">¶</a></li><li><a href=\"#5.3.-Suggestion-for-future-research\">5.3. Suggestion for future research</a><a class=\"anchor-link\" href=\"#5.3.-Suggestion-for-future-research\">¶</a></li><ul class=\"toc\"><li><a href=\"#5.3.1.-Aggregate-features\">5.3.1. Aggregate features</a><a class=\"anchor-link\" href=\"#5.3.1.-Aggregate-features\">¶</a></li></ul></ul></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project guide: https://www.dataquest.io/m/213/guided-project%3A-predicting-bike-rentals\n",
    "\n",
    "Solution by DataQuest: https://github.com/dataquestio/solutions/blob/master/Mission213Solution.ipynb\n",
    "\n",
    "In this project, I will use different machine learning algorithms (linear regression, decision trees and random forest) to predict the target and compare the output.\n",
    "\n",
    "I will use the [hourly record](https://github.com/gazay/dlnd-project-01/blob/master/Bike-Sharing-Dataset/hour.csv) of the Bike Sharing Dataset compiled by Hadi Fanaee-T. The target variable is the total number of bikes which were rented at a given hour. The following table shows a summary of the dataset, in which the \"Description\" column is an excerpt from https://github.com/gazay/dlnd-project-01/tree/master/Bike-Sharing-Dataset.\n",
    "\n",
    "|Variable|Data type|Description|Drop the feature?|Target variable?||\n",
    "|---|---|---|---|---||\n",
    "|instant|discrete|record index|Yes (non-data)||\n",
    "|dteday|ordinal|date|Yes (Uninformative)||\n",
    "|season|ordinal (circular)|season (1:springer, 2:summer, 3:fall, 4:winter)|||\n",
    "|yr|ordinal (circular)|year (0: 2011, 1:2012)|||\n",
    "|mnth|ordinal (circular)|month ( 1 to 12)|||\n",
    "|hr|ordinal (circular)|hour (0 to 23)|||\n",
    "|holiday|nominal|weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)|||\n",
    "|weekday|ordinal (circular)|day of the week|||\n",
    "|workingday|nominal|if day is neither weekend nor holiday is 1, otherwise is 0.|||\n",
    "|weathersit|nominal| 1 (Clear, Few clouds, Partly cloudy, Partly cloudy), 2 (Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist), 3 (Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds), 4 (Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog)|||\n",
    "|temp|continuous|Normalized temperature in Celsius. The values are divided to 41 (max)|||\n",
    "|atemp|continuous|Normalized feeling temperature in Celsius. The values are divided to 50 (max)|||\n",
    "|hum|continuous|Normalized humidity. The values are divided to 100 (max)|||\n",
    "|windspeed|continuous|Normalized wind speed. The values are divided to 67 (max)|||\n",
    "|casual|discrete|count of casual users|Yes (Leaks info on target variable)||\n",
    "|registered|discrete|count of registered users|Yes (Leaks info on target variable)||\n",
    "|cnt|discrete|count of total rental bikes including both casual and registered||Yes|\n",
    "\n",
    "The steps I will take are shown below.\n",
    "\n",
    "1. [Read in data](#1.-Read-in-data)<br />\n",
    "2. [Clean data](#2.-Clean-data)<br />\n",
    "    2.1. [Check for missing values](#2.1.-Check-for-missing-values)<br />\n",
    "    2.2. [Remove uninformative features](#2.2.-Remove-uninformative-features)<br />\n",
    "    2.3. [Remove features leaking info on target](#2.3.-Remove-features-leaking-info-on-target)<br />\n",
    "3. [Process features](#3.-Process-features)<br />\n",
    "    3.1. [Min-max scale continuous variables](#3.1.-Min-max-scale-continuous-variables)<br />\n",
    "    3.2. [One-hot encode nominal data](#3.2.-One-hot-encode-nominal-data)<br />\n",
    "4. [Predict and evaluate](#4.-Predict-and-evaluate)<br />\n",
    "5. [Closing remarks](#5.-Closing-remarks)<br />\n",
    "    5.1. [RMSE instead of MAE](#5.1.-RMSE-instead-of-MAE)<br />\n",
    "    5.2. [Correlated features](#5.2.-Correlated-features)<br />\n",
    "    5.3. [Suggestion for future research](#5.3.-Suggestion-for-future-research)<br />\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;5.3.1. [Aggregate features](#5.3.1.-Aggregate-features)<br />\n",
    "\n",
    "\n",
    "# 1. Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read in data\n",
    "df = pd.read_csv(\"bike_rental_hour.csv\")\n",
    "\n",
    "# Display first 5 rows\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clean data\n",
    "## 2.1. Check for missing values\n",
    "There is no missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instant       0\n",
       "dteday        0\n",
       "season        0\n",
       "yr            0\n",
       "mnth          0\n",
       "hr            0\n",
       "holiday       0\n",
       "weekday       0\n",
       "workingday    0\n",
       "weathersit    0\n",
       "temp          0\n",
       "atemp         0\n",
       "hum           0\n",
       "windspeed     0\n",
       "casual        0\n",
       "registered    0\n",
       "cnt           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Remove uninformative features\n",
    "\n",
    "`dteday` (date in year-month-date format) will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instant', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
       "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
       "       'casual', 'registered', 'cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(\"dteday\", axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`instant` (row ID) will be kept to see if bike rentals increase with over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Remove features leaking info on target\n",
    "\n",
    "`casual` and `registered` will be removed because they constitute the target variable (`cnt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instant', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
       "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop([\"casual\", \"registered\"], axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process features\n",
    "## 3.1. Min-max scale continuous variables\n",
    "\n",
    "Continouous variables are `temp` (normalised temperature), `atemp` (normalised feeling temperature), `hum` (normalised humidity) and `windspeed` (normalised wind speed). The current values are quotient from dividing each original values by their maximum values.\n",
    "\n",
    "These will be converted into min-max scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum values of original data (source https://bit.ly/2MAVs0K)\n",
    "# for each continuous variable\n",
    "max_vals = {\"temp\": 41, \"atemp\": 50, \"hum\": 100, \"windspeed\": 67}\n",
    "\n",
    "# Rescale values into min-max scale\n",
    "for col_name in df.select_dtypes(np.float):\n",
    "    \n",
    "    col = df[col_name]\n",
    "        \n",
    "    # Unscale values to recover original values\n",
    "    col_original = df[col_name] * max_vals[col_name]\n",
    "\n",
    "    # Min-max scale original values\n",
    "    col_minmax = (col_original - col_original.min()) / (col_original.max() - col_original.min())\n",
    "    \n",
    "    # Update data frame\n",
    "    df[col_name] = col_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. One-hot encode nominal data\n",
    "\n",
    "Create one column per category for nominal variables (`holiday`, `workingday`, `weathersit`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>weekday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>holiday_0</th>\n",
       "      <th>holiday_1</th>\n",
       "      <th>workingday_0</th>\n",
       "      <th>workingday_1</th>\n",
       "      <th>weathersit_1</th>\n",
       "      <th>weathersit_2</th>\n",
       "      <th>weathersit_3</th>\n",
       "      <th>weathersit_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant  season  yr  mnth  hr  weekday      temp   atemp   hum  windspeed  \\\n",
       "0        1       1   0     1   0        6  0.224490  0.2879  0.81        0.0   \n",
       "1        2       1   0     1   1        6  0.204082  0.2727  0.80        0.0   \n",
       "2        3       1   0     1   2        6  0.204082  0.2727  0.80        0.0   \n",
       "3        4       1   0     1   3        6  0.224490  0.2879  0.75        0.0   \n",
       "4        5       1   0     1   4        6  0.224490  0.2879  0.75        0.0   \n",
       "\n",
       "   cnt  holiday_0  holiday_1  workingday_0  workingday_1  weathersit_1  \\\n",
       "0   16          1          0             1             0             1   \n",
       "1   40          1          0             1             0             1   \n",
       "2   32          1          0             1             0             1   \n",
       "3   13          1          0             1             0             1   \n",
       "4    1          1          0             1             0             1   \n",
       "\n",
       "   weathersit_2  weathersit_3  weathersit_4  \n",
       "0             0             0             0  \n",
       "1             0             0             0  \n",
       "2             0             0             0  \n",
       "3             0             0             0  \n",
       "4             0             0             0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [\"holiday\", \"workingday\", \"weathersit\"]\n",
    "\n",
    "# Create dummy columns for nominal columns\n",
    "# (Values are first converted into strings because \n",
    "# pandas' get_dummies function cannot process numbers)\n",
    "dummy_cols = pd.get_dummies(df[cat_cols].astype(str))\n",
    "\n",
    "# Remove original nominal columns\n",
    "df.drop(cat_cols, axis=1, inplace=True)\n",
    "\n",
    "# Add dummy columns to data frame\n",
    "df = pd.concat([df, dummy_cols], axis=1)\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Incomplete code block for feature hashing\n",
    "# which I hope to further work on another time\n",
    "#\n",
    "# df_cats = df.select_dtypes(include=\"category\")\n",
    "# h = FeatureHasher(n_features=10, input_type=\"string\")\n",
    "# pd.DataFrame(h.transform(df_cats).toarray())\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict and evaluate\n",
    "\n",
    "* *The text in this section and the codes in the following cell are modified versions of section \"6. Predict and Evaluate\" of my [other project](https://github.com/gknam/projects/blob/master/DataScience/DataQuest/Step6_MachineLearning/4_LinearRegressionForMachineLearning/project1/PredictingHouseSalePrices.ipynb).*\n",
    "\n",
    "The following procedures will be carried out with different learning algorithms: linear regression, decision trees and random forest.\n",
    "\n",
    "___\n",
    "**K-fold cross validation** will be carried out where K will range from 2 to and including 10.\n",
    "\n",
    "In each fold, **feature selection** will be done based on each feature's importance which will be evaluated using **extremely randomised trees** algorithm ([ExtraTreesRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)).\n",
    "\n",
    "Feature selection will be done 10 times per fold. First selection will include the most important feature. The second selection will be the first selection plus the next most important feature. The same will be done for up to a selection of 10 most important features.\n",
    "\n",
    "With each set of selected features, predictions will be made using the chosen learning algorithm. Then prediction errors will be measured using [root-mean-square error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) and averaged within each fold.\n",
    "\n",
    "A total of 100 averaged MAEs will be produced, each representing a unique combination of feature selection sizes (n=10) and folds (n=10). Finally, the combination with the lowest MAE will be selected and its MAE will be reported together with the names and numbers of selected features.\n",
    "___\n",
    "\n",
    "**Note**: Feature selection is done ***during*** cross validation rather than beforehand. [This will prevent bias which can be created from using feature sets selected from the *whole* dataset (all rows) to make prediction on *subsets* of data (subsets of rows). By doing this, cross validation assesses the **model fitting process** rather than the model itself](https://stats.stackexchange.com/a/27751)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection is made using ExtraTreesRegressor.\n",
      "K-fold cross validation is carried out with K ranging from 2 to and including 2.\n",
      "\n",
      "In each fold, N most important features are selected to train the model\n",
      "where N ranges from 1 to and including 11\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LinearRegression: \n",
      "Duration: 306.32 seconds\n",
      "\n",
      "Best prediction was made with RMSE 141.84283731934252\n",
      "This was achieved in\n",
      "(1) 10-fold cross-validation\n",
      "(2) with 10 most important features selected in each split of the fold, which are listed below.\n",
      "(There may be multiple sets because feature selection was made for each split)\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'hum', 'workingday_0', 'workingday_1', 'season', 'instant', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_0', 'workingday_1', 'hum', 'instant', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_1', 'workingday_0', 'instant', 'hum', 'season', 'mnth' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'yr', 'hum', 'workingday_0', 'workingday_1', 'instant', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'yr', 'workingday_1', 'instant', 'workingday_0', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'workingday_0', 'yr', 'workingday_1', 'instant', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'yr', 'workingday_0', 'workingday_1', 'instant', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'hum', 'atemp', 'yr', 'workingday_0', 'instant', 'workingday_1', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'yr', 'temp', 'instant', 'workingday_0', 'workingday_1', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_1', 'workingday_0', 'season', 'instant', 'hum', 'weathersit_3'\n",
      "\n",
      "\n",
      "\n",
      "DecisionTreeRegressor: \n",
      "Duration: 326.23 seconds\n",
      "\n",
      "Best prediction was made with RMSE 58.24430776970852\n",
      "This was achieved in\n",
      "(1) 8-fold cross-validation\n",
      "(2) with 10 most important features selected in each split of the fold, which are listed below.\n",
      "(There may be multiple sets because feature selection was made for each split)\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_1', 'workingday_0', 'season', 'hum', 'instant', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_1', 'workingday_0', 'hum', 'instant', 'season', 'mnth' and\n",
      "\n",
      "'hr', 'atemp', 'yr', 'temp', 'workingday_1', 'instant', 'hum', 'workingday_0', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'yr', 'instant', 'workingday_0', 'hum', 'workingday_1', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'atemp', 'yr', 'hum', 'workingday_0', 'workingday_1', 'instant', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'instant', 'atemp', 'yr', 'hum', 'workingday_0', 'workingday_1', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'workingday_1', 'atemp', 'instant', 'workingday_0', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_0', 'workingday_1', 'season', 'instant', 'hum', 'weathersit_3'\n",
      "\n",
      "\n",
      "\n",
      "RandomForestRegressor: \n",
      "Duration: 470.82 seconds\n",
      "\n",
      "Best prediction was made with RMSE 45.55741931852344\n",
      "This was achieved in\n",
      "(1) 10-fold cross-validation\n",
      "(2) with 10 most important features selected in each split of the fold, which are listed below.\n",
      "(There may be multiple sets because feature selection was made for each split)\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_1', 'workingday_0', 'hum', 'instant', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'hum', 'workingday_1', 'workingday_0', 'season', 'instant', 'mnth' and\n",
      "\n",
      "'hr', 'atemp', 'yr', 'temp', 'workingday_1', 'workingday_0', 'instant', 'season', 'hum', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'yr', 'workingday_1', 'workingday_0', 'instant', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'atemp', 'yr', 'hum', 'instant', 'workingday_1', 'workingday_0', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'temp', 'yr', 'hum', 'workingday_0', 'instant', 'workingday_1', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'atemp', 'yr', 'instant', 'workingday_1', 'workingday_0', 'hum', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'temp', 'yr', 'atemp', 'workingday_0', 'hum', 'workingday_1', 'instant', 'season', 'weathersit_3' and\n",
      "\n",
      "'hr', 'atemp', 'yr', 'temp', 'workingday_0', 'workingday_1', 'instant', 'hum', 'season', 'weathersit_3'\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(selector, features, target, top_features_number):\n",
    "\n",
    "    selector.fit(features, target)\n",
    "    \n",
    "    feature_cols_series = pd.Series(features.columns, \\\n",
    "                                    name=\"Feature_cols\")\n",
    "    \n",
    "    if type(selector) == ExtraTreesRegressor:\n",
    "        feature_significance = selector.feature_importances_\n",
    "        feature_significance_label = \"Feature_importance_score\"\n",
    "        ascending = False\n",
    "\n",
    "    elif type(selector) == RFE:\n",
    "        feature_significance = selector.ranking_\n",
    "        feature_significance_label = \"Feature_importance_ranking\"\n",
    "        ascending = True\n",
    "        \n",
    "    \n",
    "    feature_significance_series = pd.Series(feature_significance, \\\n",
    "                                          name=feature_significance_label)\n",
    "\n",
    "    feature_cols_top = pd.concat([feature_cols_series, feature_significance_series], axis=1)\\\n",
    "            .sort_values(by=feature_significance_label, ascending=ascending)\\\n",
    "            .iloc[:top_features_number, 0].values\n",
    "            \n",
    "    return feature_cols_top\n",
    "\n",
    "def get_model_name(model):\n",
    "    model_type_str = str((type(model)))\n",
    "    model_name = re.sub(\".*\\.|'.*$\", \"\", model_type_str)\n",
    "    \n",
    "    return model_name\n",
    "\n",
    "def train_and_test(fs_train, fs_test, t_train, t_test, model, error_metric):\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(fs_train, t_train)\n",
    "    \n",
    "    # Predict target using test dataset\n",
    "    p_test = model.predict(fs_test)\n",
    "    \n",
    "    # Get error\n",
    "    \n",
    "    # RMSE (root-mean-square error)\n",
    "    if error_metric == \"RMSE\":\n",
    "        err = np.sqrt(mean_squared_error(t_test, p_test))\n",
    "    \n",
    "    # MAE (mean absolute error)\n",
    "    elif error_metric == \"MAE\":\n",
    "        err = np.sqrt(mean_absolute_error(t_test, p_test))\n",
    "            \n",
    "    return err\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def k_fold_cross_val(target_col, model, error_metric, selector, num_folds, feature_set_sizes):\n",
    "    # Get features\n",
    "    feature_cols = df.columns.drop(target_col)\n",
    "\n",
    "    features = df[feature_cols]\n",
    "    target = df[target_col]\n",
    "\n",
    "    # Track lowest error\n",
    "    lowest_err = df.max().max() - df.min().min()\n",
    "\n",
    "    # Track best method\n",
    "    # [fold, number of selected features, \n",
    "    # names of selected features, lowest error]\n",
    "    best_method = [None, None, None, lowest_err]\n",
    "\n",
    "\n",
    "    start = time()\n",
    "    err_all = {str(fold) + \"-fold CV\": [] for fold in num_folds}\n",
    "\n",
    "    # K-fold cross validation\n",
    "    for fold in num_folds:\n",
    "\n",
    "        err_split = []\n",
    "        kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "        err_selections = {}\n",
    "        feature_selections = {}\n",
    "\n",
    "        kf_splits = kf.split(features)\n",
    "\n",
    "        # For each fold\n",
    "        for (train_ind, test_ind), split in zip(kf_splits, range(fold)):\n",
    "\n",
    "            err_selections[split] = []\n",
    "            feature_selections[split] = []\n",
    "\n",
    "            # For each feature set\n",
    "            for fss in feature_set_sizes:\n",
    "\n",
    "                # Get target\n",
    "                t_train = target.loc[train_ind]\n",
    "                t_test = target.loc[test_ind]\n",
    "\n",
    "                # Get features\n",
    "                f_train = features.loc[train_ind]\n",
    "                f_test = features.loc[test_ind]\n",
    "\n",
    "                # Select features\n",
    "                fs_cols = feature_selection(selector, f_train, t_train, top_features_number=fss)\n",
    "\n",
    "                fs_train = f_train[fs_cols]\n",
    "                fs_test = f_test[fs_cols]\n",
    "\n",
    "                # record error and selected features\n",
    "                err = train_and_test(fs_train, fs_test, t_train, t_test, model, error_metric)\n",
    "                err_selections[split].append(err)\n",
    "                feature_selections[split].append(fs_cols)\n",
    "\n",
    "        # Get mean error and feature names per feature selection\n",
    "        err_array = np.array([err_selections[i] for i in err_selections])\n",
    "        feature_array = np.array([feature_selections[i] for i in feature_selections])\n",
    "        for fs in feature_set_sizes:\n",
    "            err_fs_mean = np.mean(err_array[:, fs - 1])\n",
    "            features_fs = feature_array[:, fs - 1]\n",
    "\n",
    "            if err_fs_mean < lowest_err:\n",
    "                lowest_err = err_fs_mean\n",
    "                best_method = [fold, fs, features_fs, lowest_err]\n",
    "\n",
    "            err_all[str(fold) + \"-fold CV\"].append(err_fs_mean)\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    # Get unique feature selections in best_method\n",
    "    # (source https://stackoverflow.com/a/3724558)\n",
    "    features_fs_best = [list(x) for x in set(tuple(x) for x in best_method[2])]\n",
    "\n",
    "    # State model name\n",
    "    model_name = get_model_name(model)\n",
    "    print(model_name + \": \")\n",
    "    \n",
    "    # State duration of cross-validation\n",
    "    print(\"Duration: \" + (\"{:.2f}\").format(end - start) + \" seconds\")\n",
    "    print()\n",
    "\n",
    "    # State (1) fold and feature sets with which best prediction was made\n",
    "    # and (2) details of error measure\n",
    "    print(\"Best prediction was made with {} {}\".format(error_metric, best_method[3]))\n",
    "    print(\"This was achieved in\")\n",
    "    print(\"(1) {}-fold cross-validation\".format(best_method[0]))\n",
    "    print(\"(2) with {} most important features selected in each split of the fold, \\\n",
    "which are listed below.\".format(best_method[1]))\n",
    "    print(\"(There may be multiple sets because feature selection was made for each split)\")\n",
    "    print()\n",
    "    \n",
    "    # State most important features selected for the fold in which best prediction was made\n",
    "    for ind, val in enumerate(features_fs_best):\n",
    "        val = re.sub(\"\\[|\\]\", \"\", str(val))\n",
    "        to_print = str(val) + \" and\" if ind < len(features_fs_best) - 1 \\\n",
    "                                  else str(val)\n",
    "        print(to_print)\n",
    "        print()\n",
    "    print(\"\\n\")\n",
    "\n",
    "    ## Return error metric for all attempted numbers of folds (columns) and feature set sizes (rows)\n",
    "    # err_all_df = pd.DataFrame(data=err_all, index=[str(i) + \" features\" for i in feature_set_sizes])\n",
    "    \n",
    "    # return err_all_df\n",
    "\n",
    "\n",
    "\n",
    "target_col = \"cnt\"\n",
    "models = [LinearRegression(), DecisionTreeRegressor(), RandomForestRegressor()]\n",
    "selector = ExtraTreesRegressor()\n",
    "error_metric = \"RMSE\"\n",
    "num_folds = range(2, 11)\n",
    "feature_set_sizes = range(1, 11)\n",
    "\n",
    "## Initialise RFE feature selector\n",
    "# estimator = SVR(kernel=\"linear\")\n",
    "# selector = RFE(estimator, 1, step=1)\n",
    "\n",
    "# State feature selector name\n",
    "selector_name = get_model_name(selector)\n",
    "print(\"Feature selection is made using {}.\".format(selector_name))\n",
    "\n",
    "# State numbers of folds\n",
    "print(\"K-fold cross validation is carried out with K ranging from 2 to and including {}.\"\\\n",
    "     .format(min(num_folds), max(num_folds)))\n",
    "print()\n",
    "\n",
    "# State sizes of feature sets\n",
    "print(\"In each fold, N most important features are selected to train the model\")\n",
    "print(\"where N ranges from {} to and including {}\".format(min(feature_set_sizes), max(feature_set_sizes) + 1))\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "for model in models:\n",
    "    k_fold_cross_val(target_col, model, error_metric, selector, num_folds, feature_set_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest algorithm produced most accurate prediction (RMSE≈141.84) while decision tress resulted in slighly lower accuracy (RMSE≈58.24) and linear regression performed noticeably worse (RMSE≈45.56).\n",
    "\n",
    "On the other hand, performance speed was best with linear regression and worst with random forest.\n",
    "\n",
    "Also, including more features in the model led to better prediction accuracy for all three algorithms. Among all features, `hr` (hour of day) was most important in predicting number of bike rentals for each hour.\n",
    "\n",
    "\n",
    "# 5. Closing remarks\n",
    "\n",
    "## 5.1. RMSE instead of MAE\n",
    "\n",
    "Root-mean-square error (RMSE) and mean absolute error (MAE) measure prediction errors on the same scale. I chose RMSE so that bigger errors are given higher weight (https://stats.stackexchange.com/a/48268).\n",
    "\n",
    "## 5.2. Correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['temp', 'atemp']]\n"
     ]
    }
   ],
   "source": [
    "def get_correlated_columns(df, r_thres=0.9, p_thres=0.05, dtype=None):\n",
    "    \"\"\"\n",
    "    df: data frame\n",
    "    columns: data frame's columns to include\n",
    "    r_thres: Threshold for pearson's r value\n",
    "    p_thres: Threshold for p value of pearson's r\n",
    "    dtype: data type\n",
    "    \n",
    "    Return df after removing highly correlated columns.\n",
    "    Hihghly correlated means r > r_thres and p < p_thres.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select columns of specified data type\n",
    "    if dtype is not None:\n",
    "        cols = df.select_dtypes(dtype).columns\n",
    "\n",
    "    cols_corr_pair = [] # Pairs of correlated columns\n",
    "    cols_corr_all = set() # All correlated columns\n",
    "    \n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            \n",
    "            col1_name = cols[i]\n",
    "            col2_name = cols[j]\n",
    "            \n",
    "            col1 = df[col1_name]\n",
    "            col2 = df[col2_name]\n",
    "            r, p = pearsonr(col1, col2)\n",
    "\n",
    "            if (r > 0.9) and (p < 0.05):\n",
    "                cols_corr_pair.append([col1_name, col2_name])\n",
    "                cols_corr_all.add(col1_name)\n",
    "                cols_corr_all.add(col2_name)\n",
    "                \n",
    "    return cols_corr_pair, cols_corr_all\n",
    "\n",
    "cols_corr_pair, cols_corr_all = get_correlated_columns(df.drop(\"cnt\", axis=1), dtype=np.float)\n",
    "print(cols_corr_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature (`temp`) and feeling temperature (`atemp`) are highly (r > 0.9) and significantly (p < 0.05) correlated. This can affect calculation of feature importance in random forest algorithm (https://stats.stackexchange.com/a/144732/209342). This is an issue for this project because extremely random trees, which I used for feature selection, is somewhat similar to random forest.\n",
    "\n",
    "One alternative could be to do feature selection using recursive feature elimination (RFE) algorithm which could reduce such effect ([Gregorutti et al., 2017](https://arxiv.org/abs/1310.5726)).\n",
    "\n",
    "## 5.3. Suggestion for future research\n",
    "### 5.3.1. Aggregate features\n",
    "\n",
    "One could investigate the interaction between features. For example, one could check if feeling temperature (`atemp`) could affect the number of bike rentals differently depending on `windspeed`. A new variable could be created for this by multiplying the two variables together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
