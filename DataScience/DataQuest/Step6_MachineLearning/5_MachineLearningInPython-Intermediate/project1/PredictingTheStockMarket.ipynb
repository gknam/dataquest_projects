{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<br />\n",
    "<div id=\"toc\"><ul class=\"toc\"><li><a href=\"#1.-Read-in-data\">1. Read in data</a><a class=\"anchor-link\" href=\"#1.-Read-in-data\">¶</a></li><li><a href=\"#2.-Check-for-missing-data\">2. Check for missing data</a><a class=\"anchor-link\" href=\"#2.-Check-for-missing-data\">¶</a></li><li><a href=\"#3.-Feature-processing\">3. Feature processing</a><a class=\"anchor-link\" href=\"#3.-Feature-processing\">¶</a></li><ul class=\"toc\"><li><a href=\"#3.1.-Create-new-features-by-separating-or-aggregating-existing-features\">3.1. Create new features by separating or aggregating existing features</a><a class=\"anchor-link\" href=\"#3.1.-Create-new-features-by-separating-or-aggregating-existing-features\">¶</a></li><li><a href=\"#3.2.-Drop-observations-with-insufficient-past-data\">3.2. Drop observations with insufficient past data</a><a class=\"anchor-link\" href=\"#3.2.-Drop-observations-with-insufficient-past-data\">¶</a></li><li><a href=\"#3.3.-Min-max-scale-all-numerical-features\">3.3. Min-max scale all numerical features</a><a class=\"anchor-link\" href=\"#3.3.-Min-max-scale-all-numerical-features\">¶</a></li><li><a href=\"#3.4.-Drop-correlated-features\">3.4. Drop correlated features</a><a class=\"anchor-link\" href=\"#3.4.-Drop-correlated-features\">¶</a></li></ul><li><a href=\"#4.-Predict-and-evaluate\">4. Predict and evaluate</a><a class=\"anchor-link\" href=\"#4.-Predict-and-evaluate\">¶</a></li><li><a href=\"#5.-Closing-remarks\">5. Closing remarks</a><a class=\"anchor-link\" href=\"#5.-Closing-remarks\">¶</a></li><ul class=\"toc\"><li><a href=\"#5.1.-Limitation\">5.1. Limitation</a><a class=\"anchor-link\" href=\"#5.1.-Limitation\">¶</a></li><ul class=\"toc\"><li><a href=\"#5.1.2.-Feature-selection-method\">5.1.2. Feature selection method</a><a class=\"anchor-link\" href=\"#5.1.2.-Feature-selection-method\">¶</a></li><li><a href=\"#5.1.3.-Activation-function\">5.1.3. Activation function</a><a class=\"anchor-link\" href=\"#5.1.3.-Activation-function\">¶</a></li></ul></ul></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/m/65/guided-project%3A-predicting-the-stock-market\n",
    "\n",
    "In this project, I will predict each day's closing stock price for [S&P 500 Index](https://en.wikipedia.org/wiki/S%26P_500_Index) based on the past record. I will use neural networks to train the model with records from `1950-2012`, and make predictions for `2013-2015`.\n",
    "\n",
    "The dataset has been prepared by DataQuest, whose description of each column is as follows.\n",
    "___\n",
    "*   `Date` \\-\\- The date of the record.\n",
    "*   `Open` \\-\\- The opening price of the day (when trading starts).\n",
    "*   `High` \\-\\- The highest trade price during the day.\n",
    "*   `Low` \\-\\- The lowest trade price during the day.\n",
    "*   `Volume` \\-\\- The number of shares traded.\n",
    "*   `Adj Close` \\-\\- The daily closing price, adjusted retroactively to include any corporate actions. Read more [here](http://www.investopedia.com/terms/a/adjusted_closing_price.asp).\n",
    "*   `Close` (**target variable**) \\-\\- The closing price for the day (when trading is finished).\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16590 entries, 0 to 16589\n",
      "Data columns (total 7 columns):\n",
      "Date         16590 non-null object\n",
      "Open         16590 non-null float64\n",
      "High         16590 non-null float64\n",
      "Low          16590 non-null float64\n",
      "Close        16590 non-null float64\n",
      "Volume       16590 non-null float64\n",
      "Adj Close    16590 non-null float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 907.3+ KB\n",
      "None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-07</td>\n",
       "      <td>2090.419922</td>\n",
       "      <td>2090.419922</td>\n",
       "      <td>2066.780029</td>\n",
       "      <td>2077.070068</td>\n",
       "      <td>4.043820e+09</td>\n",
       "      <td>2077.070068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-04</td>\n",
       "      <td>2051.239990</td>\n",
       "      <td>2093.840088</td>\n",
       "      <td>2051.239990</td>\n",
       "      <td>2091.689941</td>\n",
       "      <td>4.214910e+09</td>\n",
       "      <td>2091.689941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>2080.709961</td>\n",
       "      <td>2085.000000</td>\n",
       "      <td>2042.349976</td>\n",
       "      <td>2049.620117</td>\n",
       "      <td>4.306490e+09</td>\n",
       "      <td>2049.620117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-02</td>\n",
       "      <td>2101.709961</td>\n",
       "      <td>2104.270020</td>\n",
       "      <td>2077.110107</td>\n",
       "      <td>2079.510010</td>\n",
       "      <td>3.950640e+09</td>\n",
       "      <td>2079.510010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>2082.929932</td>\n",
       "      <td>2103.370117</td>\n",
       "      <td>2082.929932</td>\n",
       "      <td>2102.629883</td>\n",
       "      <td>3.712120e+09</td>\n",
       "      <td>2102.629883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2015-12-07  2090.419922  2090.419922  2066.780029  2077.070068   \n",
       "1  2015-12-04  2051.239990  2093.840088  2051.239990  2091.689941   \n",
       "2  2015-12-03  2080.709961  2085.000000  2042.349976  2049.620117   \n",
       "3  2015-12-02  2101.709961  2104.270020  2077.110107  2079.510010   \n",
       "4  2015-12-01  2082.929932  2103.370117  2082.929932  2102.629883   \n",
       "\n",
       "         Volume    Adj Close  \n",
       "0  4.043820e+09  2077.070068  \n",
       "1  4.214910e+09  2091.689941  \n",
       "2  4.306490e+09  2049.620117  \n",
       "3  3.950640e+09  2079.510010  \n",
       "4  3.712120e+09  2102.629883  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from time import time\n",
    "\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sphist.csv\")\n",
    "\n",
    "# Get names of original columns\n",
    "original_cols = df.columns\n",
    "\n",
    "# Set target column (closing price of the day)\n",
    "target_col = \"Close\"\n",
    "\n",
    "# Dataset summary\n",
    "print(df.info())\n",
    "print()\n",
    "\n",
    "# Display first 5 rows\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Check for missing data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing data point per feature:\n",
      "Date         0\n",
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Volume       0\n",
      "Adj Close    0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data\n",
    "print(\"Number of missing data point per feature:\")\n",
    "print(df.isna().sum())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature processing\n",
    "\n",
    "The code cell below will do the following.\n",
    "\n",
    "## 3.1. Create new features by separating or aggregating existing features\n",
    "\n",
    "`Date` feature will be separated into `Year`, `Month` and `Day` features.\n",
    "\n",
    "`Close` and `Volume` features will be aggregated. For each day, N past days' records will be gathered excluding the day (e.g. 5 past days from 2012-12-31 will refer to 2012-12-26 to and including 2012-12-30). The following sets of features will be created for each.\n",
    "\n",
    "* The mean and standard deviation of values over the past N days, where N days refer to ...\n",
    "    * 5 past working days\n",
    "    * 30 past working days\n",
    "    * 365 past days (including holidays)\n",
    "\n",
    "* Ratios of means and standard deviations between ...\n",
    "    * 5 past working days / 30 past working days\n",
    "    * 5 past working days / 365 past days (including holidays)\n",
    "    \n",
    "## 3.2. Drop observations with insufficient past data\n",
    "\n",
    "Aggregated features from previous section would require past data of up to 365 past days. This means that the first 365 days' records will not have past record, and therefore they will be removed from analysis.\n",
    "\n",
    "## 3.3. Min-max scale all numerical features\n",
    "\n",
    "All neumerical features will be transformed into min-max range (https://en.wikipedia.org/wiki/Feature_scaling#Rescaling ). This will put all values into [0, 1] range.\n",
    "\n",
    "* See [last section](#4.1.2.-Activation-function) for why min-max scaling will be used instead of [standardisation](https://en.wikipedia.org/wiki/Standard_score#Calculation_from_raw_score).\n",
    "\n",
    "## 3.4. Drop correlated features\n",
    "\n",
    "In case two features were highly correlated (Pearson r > 0.95), either one of them was removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in date: 1950-01-03 00:00:00\n",
      "First date to include in training data 1951-01-03 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Close: Past 5 days mean</th>\n",
       "      <th>Close: Past 5 days SD</th>\n",
       "      <th>Close: Past 30 days SD</th>\n",
       "      <th>Close means ratio: past 5 days / past 30 days</th>\n",
       "      <th>Close SDs ratio: past 5 days / past 30 days</th>\n",
       "      <th>Close means ratio: past 5 days / past 365 days</th>\n",
       "      <th>Close SDs ratio: past 5 days / past 365 days</th>\n",
       "      <th>Volume: Past 5 days mean</th>\n",
       "      <th>Volume: Past 365 days mean</th>\n",
       "      <th>Volume: Past 5 days SD</th>\n",
       "      <th>Volume: Past 30 days SD</th>\n",
       "      <th>Volume means ratio: past 5 days / past 30 days</th>\n",
       "      <th>Volume SDs ratio: past 5 days / past 30 days</th>\n",
       "      <th>Volume means ratio: past 5 days / past 365 days</th>\n",
       "      <th>Volume SDs ratio: past 5 days / past 365 days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.803896</td>\n",
       "      <td>0.374452</td>\n",
       "      <td>0.023061</td>\n",
       "      <td>0.416674</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.189881</td>\n",
       "      <td>0.047575</td>\n",
       "      <td>0.112902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.825820</td>\n",
       "      <td>0.262149</td>\n",
       "      <td>0.024406</td>\n",
       "      <td>0.317158</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.528329</td>\n",
       "      <td>0.124929</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>0.078290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.836977</td>\n",
       "      <td>0.224613</td>\n",
       "      <td>0.025173</td>\n",
       "      <td>0.289886</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.535468</td>\n",
       "      <td>0.080559</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.054757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.848501</td>\n",
       "      <td>0.190383</td>\n",
       "      <td>0.026010</td>\n",
       "      <td>0.263050</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.498266</td>\n",
       "      <td>0.140983</td>\n",
       "      <td>0.048566</td>\n",
       "      <td>0.084911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.862964</td>\n",
       "      <td>0.137202</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.203936</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.502812</td>\n",
       "      <td>0.187030</td>\n",
       "      <td>0.049640</td>\n",
       "      <td>0.111325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close  Year  Month       Day  Close: Past 5 days mean  \\\n",
       "250  0.000000   0.0    0.0  0.066667                 0.000000   \n",
       "251  0.000085   0.0    0.0  0.100000                 0.000075   \n",
       "252  0.000085   0.0    0.0  0.133333                 0.000120   \n",
       "253  0.000147   0.0    0.0  0.233333                 0.000169   \n",
       "254  0.000204   0.0    0.0  0.266667                 0.000224   \n",
       "\n",
       "     Close: Past 5 days SD  Close: Past 30 days SD  \\\n",
       "250               0.002976                0.002239   \n",
       "251               0.002247                0.002536   \n",
       "252               0.002058                0.002793   \n",
       "253               0.001871                0.003083   \n",
       "254               0.001430                0.003388   \n",
       "\n",
       "     Close means ratio: past 5 days / past 30 days  \\\n",
       "250                                       0.803896   \n",
       "251                                       0.825820   \n",
       "252                                       0.836977   \n",
       "253                                       0.848501   \n",
       "254                                       0.862964   \n",
       "\n",
       "     Close SDs ratio: past 5 days / past 30 days  \\\n",
       "250                                     0.374452   \n",
       "251                                     0.262149   \n",
       "252                                     0.224613   \n",
       "253                                     0.190383   \n",
       "254                                     0.137202   \n",
       "\n",
       "     Close means ratio: past 5 days / past 365 days  \\\n",
       "250                                        0.023061   \n",
       "251                                        0.024406   \n",
       "252                                        0.025173   \n",
       "253                                        0.026010   \n",
       "254                                        0.026949   \n",
       "\n",
       "     Close SDs ratio: past 5 days / past 365 days  Volume: Past 5 days mean  \\\n",
       "250                                      0.416674                  0.000264   \n",
       "251                                      0.317158                  0.000277   \n",
       "252                                      0.289886                  0.000286   \n",
       "253                                      0.263050                  0.000271   \n",
       "254                                      0.203936                  0.000278   \n",
       "\n",
       "     Volume: Past 365 days mean  Volume: Past 5 days SD  \\\n",
       "250                    0.000517                0.000133   \n",
       "251                    0.000523                0.000093   \n",
       "252                    0.000530                0.000065   \n",
       "253                    0.000534                0.000101   \n",
       "254                    0.000543                0.000135   \n",
       "\n",
       "     Volume: Past 30 days SD  Volume means ratio: past 5 days / past 30 days  \\\n",
       "250                 0.000286                                        0.510870   \n",
       "251                 0.000288                                        0.528329   \n",
       "252                 0.000290                                        0.535468   \n",
       "253                 0.000283                                        0.498266   \n",
       "254                 0.000295                                        0.502812   \n",
       "\n",
       "     Volume SDs ratio: past 5 days / past 30 days  \\\n",
       "250                                      0.189881   \n",
       "251                                      0.124929   \n",
       "252                                      0.080559   \n",
       "253                                      0.140983   \n",
       "254                                      0.187030   \n",
       "\n",
       "     Volume means ratio: past 5 days / past 365 days  \\\n",
       "250                                         0.047575   \n",
       "251                                         0.050075   \n",
       "252                                         0.051535   \n",
       "253                                         0.048566   \n",
       "254                                         0.049640   \n",
       "\n",
       "     Volume SDs ratio: past 5 days / past 365 days  \n",
       "250                                       0.112902  \n",
       "251                                       0.078290  \n",
       "252                                       0.054757  \n",
       "253                                       0.084911  \n",
       "254                                       0.111325  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_correlated_features(df, corr_thres, target_col):\n",
    "    \"\"\"\n",
    "    The code is from https://bit.ly/2J4WkIw\n",
    "    \n",
    "    df: Data frame\n",
    "    corr_thres: Upper limit of correlation between features.\n",
    "                If two features are correlated above corr_thes,\n",
    "                one of them will be removed.\n",
    "    target_col: Target column which will be excluded from\n",
    "                correlation coefficient calculation\n",
    "    \n",
    "    Return data frame after removing features\n",
    "    which are correlated together beyond corr_thres\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Identify Highly Correlated Features\n",
    "    # Create correlation matrix with feature columns\n",
    "    corr_matrix = df.drop(target_col, axis=1).corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > corr_thres)]\n",
    "    \n",
    "    ## Drop Marked Features\n",
    "    # Drop features\n",
    "    df = df.drop(df[to_drop].columns, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_scaling(df, mode=\"minmax\"):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    mode: Method of scaling.\n",
    "        \"standardise\" converts values to standard scores.\n",
    "        \"minmax\" puts all values into [0, 1] range.\n",
    "    \n",
    "    Return df after replacing values in numerical columns\n",
    "    with scaled values.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        series = df[col]\n",
    "        if mode == \"standardise\":    \n",
    "            df[col] = (series - np.mean(series)) / np.std(series)\n",
    "        elif mode == \"minmax\":\n",
    "            df[col] = (series - np.min(series)) / (np.max(series) - np.min(series))\n",
    "        \n",
    "    return df\n",
    "\n",
    "def feature_aggregation_separation(df, max_offset, first_ind_to_include):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    first_ind_to_include: Index for the first row to include in\n",
    "                          training and testing sets\n",
    "    max_offset: Training and testing sets will exclude data\n",
    "                from the first date in data up to max_offset days.\n",
    "    \n",
    "    Take rows to be included in training and testing sets.\n",
    "    \n",
    "    Add new features which have been extracted or aggregated\n",
    "    from existing features.\n",
    "    \n",
    "    Return df.\n",
    "    \"\"\"\n",
    "\n",
    "    # For all rows to be included in training and testing\n",
    "    for ind, row in df.loc[first_ind_to_include:].iterrows():\n",
    "\n",
    "        # Add new columns for year, month, and day of week\n",
    "        ymd = row[\"Date\"]\n",
    "        \n",
    "        df.loc[ind, \"Year\"] = ymd.year\n",
    "        df.loc[ind, \"Month\"] = ymd.month\n",
    "        df.loc[ind, \"Day\"] = ymd.day\n",
    "\n",
    "        # For Close (closing price) and Volume columns\n",
    "        for col in [\"Close\", \"Volume\"]:\n",
    "\n",
    "            # Get values for the past (1) 5 and 30 trading days\n",
    "            # and (2) max_offset days including holidays\n",
    "            ind_5 = slice(ind - 5, ind)\n",
    "            ind_30 = slice(ind - 30, ind)\n",
    "            ind_max = slice(df[df[\"Date\"] <= ymd - pd.DateOffset(days=max_offset)].index[0], ind)\n",
    "\n",
    "            val_5 = df.loc[ind_5, \"{}\".format(col)]\n",
    "            val_30 = df.loc[ind_30, \"{}\".format(col)]\n",
    "            val_max = df.loc[ind_max, \"{}\".format(col)]\n",
    "\n",
    "            # Add new columns of ...\n",
    "            ## Value mean\n",
    "            val_5_mean = np.mean(val_5)\n",
    "            val_30_mean = np.mean(val_30)\n",
    "            val_max_mean = np.mean(val_max)\n",
    "\n",
    "            df.loc[ind, \"{}: Past 5 days mean\".format(col)] = val_5_mean\n",
    "            df.loc[ind, \"{}: Past 30 days mean\".format(col)] = val_30_mean\n",
    "            df.loc[ind, \"{}: Past {} days mean\".format(col, max_offset)] = val_max_mean\n",
    "\n",
    "            ## Value SD\n",
    "            val_5_sd = np.std(val_5)\n",
    "            val_30_sd = np.std(val_30)\n",
    "            val_max_sd = np.std(val_max)\n",
    "\n",
    "            df.loc[ind, \"{}: Past 5 days SD\".format(col)] = val_5_sd\n",
    "            df.loc[ind, \"{}: Past 30 days SD\".format(col)] = val_30_sd\n",
    "            df.loc[ind, \"{}: Past {} days SD\".format(col, max_offset)] = val_max_sd\n",
    "\n",
    "            ## Ratios between statistics from different periods in the past\n",
    "            ### 5 days vs 30 days\n",
    "            df.loc[ind, \"{} means ratio: past 5 days / past 30 days\".format(col)] = val_5_mean / val_30_mean\n",
    "            df.loc[ind, \"{} SDs ratio: past 5 days / past 30 days\".format(col)] = val_5_sd / val_30_sd\n",
    "\n",
    "            ### 5 days and max_offset days\n",
    "            df.loc[ind, \"{} means ratio: past 5 days / past {} days\".format(col, max_offset)] = val_5_mean / val_max_mean\n",
    "            df.loc[ind, \"{} SDs ratio: past 5 days / past {} days\".format(col, max_offset)] = val_5_sd / val_max_sd\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_excluded(df, first_ind_to_include, original_cols, target_col):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    first_ind_to_include: Index of first row to include\n",
    "    original_cols: Original columns which will be removed to avoid data leakage\n",
    "                   (I am trying to predict each day's value based on info\n",
    "                   from PREVIOUS DAYS. Therefore, any info from each day, which\n",
    "                   the original columns contain, should not be input into\n",
    "                   the learning algorighm.)\n",
    "    target_col: Target column which contains values to be predicted\n",
    "    \"\"\"\n",
    "    # Remove rows to be excluded from training and testing sets\n",
    "    df = df.loc[first_ind_to_include:]\n",
    "\n",
    "    # Remove original columns to avoid data leakage\n",
    "    df = df.drop(original_cols.drop(target_col), axis=1)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def feature_processing(df, max_offset, original_cols, target_col, corr_thres):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    max_offset: Training and testing sets will exclude the data\n",
    "                    up to [first date in data + max_offset days]\n",
    "    original_cols: Columns from original dataset.\n",
    "    target_col: Target column which contains values to be predicted\n",
    "    corr_thres: Upper limit of correlation between features.\n",
    "                If two features are correlated above corr_thes,\n",
    "                one of them will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get first date to include (= max_offset days after the first date in data)\n",
    "    # in training and testing sets\n",
    "    first_date_in_data = df.iloc[0][\"Date\"]\n",
    "    first_date_to_include = first_date_in_data + pd.DateOffset(days=max_offset)\n",
    "    first_ind_to_include = df.index[(df[\"Date\"] >= first_date_to_include)][0]\n",
    "\n",
    "    print(\"First date in date:\", first_date_in_data)\n",
    "    print(\"First date to include in training data\", first_date_to_include)\n",
    "\n",
    "    # Create aggregated features for training and testing\n",
    "    df = feature_aggregation_separation(df, max_offset, first_ind_to_include)\n",
    "    \n",
    "    # Remove rows and columns to be excluded from training and testing sets\n",
    "    df = drop_excluded(df, first_ind_to_include, original_cols, target_col)\n",
    "\n",
    "    # Scale numerical features\n",
    "    df = feature_scaling(df, mode=\"minmax\")\n",
    "\n",
    "    # Drop features which are highly correlated with one another\n",
    "    df = drop_correlated_features(df, corr_thres, target_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Format Date column\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Sort data frame by Date column and reset index\n",
    "df = df.sort_values(by=\"Date\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Feature processing - create new columns based on existing ones\n",
    "df = feature_processing(df, 365, original_cols, target_col, corr_thres=0.95)\n",
    "\n",
    "# Display first 5 rows of data frame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict and evaluate\n",
    "\n",
    "* *The text in this section and the codes in the following cell are modified versions of section \"6. Predict and Evaluate\" of my [other project](https://github.com/gknam/projects/blob/master/DataScience/DataQuest/Step6_MachineLearning/4_LinearRegressionForMachineLearning/project1/PredictingHouseSalePrices.ipynb).*\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "**K-fold cross validation** will be carried out where K will range from 2 to and including 10.\n",
    "\n",
    "In each fold, **feature selection** will be done based on each feature's importance which will be evaluated using **extremely randomised trees** algorithm ([ExtraTreesRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor)).\n",
    "\n",
    "Feature selection will be done 15 times per fold. First selection will include the most important feature. The second selection will be the first selection plus the next most important feature. The same will be done for up to a selection of 10 most important features.\n",
    "\n",
    "Neural networks will be trained with each set of selected features. Then predictions will be made, of which errors will be measured using [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE) and averaged within each feature selection.\n",
    "\n",
    "A total of 100 averaged MAEs will be produced, each representing a unique combination of feature selection sizes (n=10) and folds (n=10). Finally, the combination with the lowest MAE will be selected and its MAE will be reported together with the names and numbers of selected features.\n",
    "___\n",
    "\n",
    "**Note**: Feature selection is done ***during*** cross validation rather than beforehand. [This will prevent bias which can be created from using feature sets selected from the *whole* dataset (all rows) to make prediction on *subsets* of data (subsets of rows). By doing this, cross validation assesses the **model fitting process** rather than the model itself](https://stats.stackexchange.com/a/27751)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 4294.38 seconds\n",
      "\n",
      "K-fold cross validation was tried with K ranging from 2 to and including 9.\n",
      "\n",
      "Feature selection was made using ExtraTreesRegressor.\n",
      "Different selection sizes were tried with\n",
      "smallest set including 1 features\n",
      "and maximum one including 15 features.\n",
      "\n",
      "Best prediction was made with MAE 0.0023255344188666703 in\n",
      "(1) 5-fold cross-validation\n",
      "(2) with 1 best features selected\n",
      "\n",
      "The best feature sets selected in each fold were\n",
      "\n",
      "'Close: Past 5 days mean'\n",
      "\n",
      "\n",
      "(Each feature set lists features in order of importance)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2-fold CV</th>\n",
       "      <th>3-fold CV</th>\n",
       "      <th>4-fold CV</th>\n",
       "      <th>5-fold CV</th>\n",
       "      <th>6-fold CV</th>\n",
       "      <th>7-fold CV</th>\n",
       "      <th>8-fold CV</th>\n",
       "      <th>9-fold CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 feature</th>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.002431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 feature</th>\n",
       "      <td>0.004019</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.002696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 feature</th>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.002680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 feature</th>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.002642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 feature</th>\n",
       "      <td>0.002876</td>\n",
       "      <td>0.002674</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 feature</th>\n",
       "      <td>0.002879</td>\n",
       "      <td>0.002657</td>\n",
       "      <td>0.002529</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.002731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 feature</th>\n",
       "      <td>0.002676</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.002677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8 feature</th>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 feature</th>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.002644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 feature</th>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 feature</th>\n",
       "      <td>0.002958</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.002994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12 feature</th>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.002961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13 feature</th>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.002979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 feature</th>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>0.003448</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.003149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15 feature</th>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.003308</td>\n",
       "      <td>0.003292</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.003613</td>\n",
       "      <td>0.003266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2-fold CV  3-fold CV  4-fold CV  5-fold CV  6-fold CV  7-fold CV  \\\n",
       "1 feature    0.002470   0.002475   0.002434   0.002326   0.002503   0.002469   \n",
       "2 feature    0.004019   0.003268   0.002961   0.002940   0.002612   0.003985   \n",
       "3 feature    0.002706   0.002744   0.002609   0.002611   0.002826   0.003126   \n",
       "4 feature    0.002592   0.002786   0.002822   0.002669   0.002508   0.002773   \n",
       "5 feature    0.002876   0.002674   0.002717   0.002632   0.002529   0.003007   \n",
       "6 feature    0.002879   0.002657   0.002529   0.002687   0.002905   0.003118   \n",
       "7 feature    0.002676   0.002923   0.002864   0.003121   0.002837   0.002986   \n",
       "8 feature    0.002734   0.003026   0.002765   0.002743   0.002906   0.002945   \n",
       "9 feature    0.003184   0.002564   0.002729   0.002699   0.002662   0.003254   \n",
       "10 feature   0.002650   0.003118   0.002629   0.002781   0.003098   0.003122   \n",
       "11 feature   0.002958   0.003118   0.002828   0.002681   0.002954   0.004351   \n",
       "12 feature   0.003088   0.003221   0.003086   0.002890   0.003097   0.004179   \n",
       "13 feature   0.003160   0.003252   0.002899   0.002898   0.003068   0.003702   \n",
       "14 feature   0.003459   0.003190   0.003448   0.003030   0.003303   0.003652   \n",
       "15 feature   0.003726   0.003276   0.003308   0.003292   0.003693   0.003459   \n",
       "\n",
       "            8-fold CV  9-fold CV  \n",
       "1 feature    0.002509   0.002431  \n",
       "2 feature    0.002836   0.002696  \n",
       "3 feature    0.002645   0.002680  \n",
       "4 feature    0.002663   0.002642  \n",
       "5 feature    0.002623   0.002663  \n",
       "6 feature    0.002734   0.002731  \n",
       "7 feature    0.002669   0.002677  \n",
       "8 feature    0.002633   0.002591  \n",
       "9 feature    0.002627   0.002644  \n",
       "10 feature   0.002592   0.002713  \n",
       "11 feature   0.002856   0.002994  \n",
       "12 feature   0.003039   0.002961  \n",
       "13 feature   0.002862   0.002979  \n",
       "14 feature   0.002995   0.003149  \n",
       "15 feature   0.003613   0.003266  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_selection(selector, features, target, top_features_number):\n",
    "\n",
    "    selector.fit(features, target)\n",
    "    \n",
    "    feature_cols_series = pd.Series(feature_cols, \\\n",
    "                                    name=\"Feature_cols\")\n",
    "    \n",
    "    if type(selector) == ExtraTreesRegressor:\n",
    "        feature_significance = selector.feature_importances_\n",
    "        feature_significance_label = \"Feature_importance_score\"\n",
    "        ascending = False\n",
    "\n",
    "    elif type(selector) == RFE:\n",
    "        feature_significance = selector.ranking_\n",
    "        feature_significance_label = \"Feature_importance_ranking\"\n",
    "        ascending = True\n",
    "        \n",
    "    \n",
    "    feature_significance_series = pd.Series(feature_significance, \\\n",
    "                                          name=feature_significance_label)\n",
    "\n",
    "    feature_cols_top = pd.concat([feature_cols_series, feature_significance_series], axis=1)\\\n",
    "            .sort_values(by=feature_significance_label, ascending=ascending)\\\n",
    "            .iloc[:top_features_number, 0].values\n",
    "            \n",
    "    return feature_cols_top\n",
    "\n",
    "def train_and_test(fs_train, fs_test, t_train, t_test, model):\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(fs_train, t_train)\n",
    "    \n",
    "    # Predict target using test dataset\n",
    "    p_test = model.predict(fs_test)\n",
    "    \n",
    "    # Get MAE (mean absolute error)\n",
    "    mae = mean_absolute_error(t_test, p_test)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "\n",
    "# Get features and target\n",
    "feature_cols = df.columns.drop(target_col)\n",
    "\n",
    "features = df[feature_cols]\n",
    "target = df[target_col]\n",
    "\n",
    "# Calculate number of neurons in hidden layer\n",
    "n_input = (features.shape[1] + 1)\n",
    "n_output = 1\n",
    "n_sample = features.shape[0]\n",
    "alpha = 2\n",
    "n_hidden = int(n_sample / (alpha * (n_input + n_output)))\n",
    "\n",
    "# Model to use for prediction\n",
    "model = MLPRegressor(hidden_layer_sizes=n_hidden)\n",
    "\n",
    "# Track lowest MAE\n",
    "lowest_mae = df.max().max() - df.min().min()\n",
    "\n",
    "# Track best method\n",
    "# [fold, number of selected features, \n",
    "# names of selected features, lowest MAE]\n",
    "best_method = [None, None, None, lowest_mae]\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "# # initiate RFE feature selector\n",
    "# estimator = SVR(kernel=\"linear\")\n",
    "# selector = RFE(estimator, 1, step=1)\n",
    "\n",
    "# Initiate ExtraTreesRegressor feature selector\n",
    "selector = ExtraTreesRegressor(n_estimators=100)\n",
    "\n",
    "num_folds = range(2, 10)\n",
    "mae_all = {str(fold) + \"-fold CV\": [] for fold in num_folds}\n",
    "feature_set_sizes = range(1, 16)\n",
    "\n",
    "# K-fold cross validation\n",
    "for fold in num_folds:\n",
    "    \n",
    "    mae_split = []\n",
    "    kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "    mae_selections = {}\n",
    "    feature_selections = {}\n",
    "    \n",
    "    kf_splits = kf.split(features)\n",
    "    \n",
    "    for (train_ind, test_ind), split in zip(kf_splits, range(fold)):\n",
    "\n",
    "        mae_selections[split] = []\n",
    "        feature_selections[split] = []\n",
    "        \n",
    "        for fss in feature_set_sizes:\n",
    "\n",
    "            # Get target\n",
    "            t_train = target.iloc[train_ind]\n",
    "            t_test = target.iloc[test_ind]\n",
    "            \n",
    "            # Get features\n",
    "            f_train = features.iloc[train_ind]\n",
    "            f_test = features.iloc[test_ind]\n",
    "            \n",
    "            # Select features\n",
    "            fs_cols = feature_selection(selector, f_train, t_train, top_features_number=fss)\n",
    "            \n",
    "            fs_train = f_train[fs_cols]\n",
    "            fs_test = f_test[fs_cols]\n",
    "            \n",
    "            # record MAE and selected features\n",
    "            mae = train_and_test(fs_train, fs_test, t_train, t_test, model)\n",
    "            mae_selections[split].append(mae)\n",
    "            feature_selections[split].append(fs_cols)\n",
    "        \n",
    "    # Get mean MAE and feature names per feature selection\n",
    "    mae_array = np.array([mae_selections[i] for i in mae_selections])\n",
    "    feature_array = np.array([feature_selections[i] for i in feature_selections])\n",
    "    for fs in feature_set_sizes:\n",
    "        mae_fs_mean = np.mean(mae_array[:, fs - 1])\n",
    "        features_fs = feature_array[:, fs - 1]\n",
    "        \n",
    "        if mae_fs_mean < lowest_mae:\n",
    "            lowest_mae = mae_fs_mean\n",
    "            best_method = [fold, fs, features_fs, lowest_mae]\n",
    "        \n",
    "        mae_all[str(fold) + \"-fold CV\"].append(mae_fs_mean)\n",
    "    \n",
    "end = time()\n",
    "\n",
    "# Get unique feature selections in best_method\n",
    "# (source https://stackoverflow.com/a/3724558)\n",
    "features_fs_all = [list(x) for x in set(tuple(x) for x in best_method[2])]\n",
    "\n",
    "# Get selector name\n",
    "selector_type_str = str((type(selector)))\n",
    "selector_name = re.sub(\".*\\.|'.*$\", \"\", selector_type_str)\n",
    "\n",
    "print(\"Duration: \" + (\"{:.2f}\").format(end - start) + \" seconds\")\n",
    "print()\n",
    "\n",
    "print(\"K-fold cross validation was tried with K ranging from {} to and including {}.\"\\\n",
    "     .format(min(num_folds), max(num_folds)))\n",
    "print()\n",
    "print(\"Feature selection was made using {}.\".format(selector_name))\n",
    "print(\"Different selection sizes were tried with\")\n",
    "print(\"smallest set including {} features\".format(min(feature_set_sizes)))\n",
    "print(\"and maximum one including {} features.\".format(max(feature_set_sizes)))\n",
    "print()\n",
    "\n",
    "print(\"Best prediction was made with MAE {} in\".format(best_method[3]))\n",
    "print(\"(1) {}-fold cross-validation\".format(best_method[0]))\n",
    "print(\"(2) with {} best features selected\".format(best_method[1]))\n",
    "print()\n",
    "print(\"The best feature sets selected in each fold were\")\n",
    "print()\n",
    "for ind, val in enumerate(features_fs_all):\n",
    "    val = re.sub(\"\\[|\\]\", \"\", str(val))\n",
    "    to_print = str(val) + \" and\" if ind < len(features_fs_all) - 1 \\\n",
    "                              else str(val)\n",
    "    \n",
    "    print(to_print)\n",
    "    print()\n",
    "print()\n",
    "print(\"(Each feature set lists features in order of importance)\")\n",
    "    \n",
    "mae_all_df = pd.DataFrame(data=mae_all, index=[str(i) + \" feature\" for i in feature_set_sizes])\n",
    "mae_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a neural network, the lowest prediction error (MAE≈0.002) was achieved when 1 most important features were used in 5-fold cross-validation.\n",
    "\n",
    "\n",
    "# 5. Closing remarks\n",
    "\n",
    "## 5.1. Limitation\n",
    "\n",
    "### 5.1.2. Feature selection method\n",
    "\n",
    "Again, the comment has been taken from section \"7.1.2. Feature selection method\" of [my other machine learning project](https://github.com/gknam/projects/blob/master/DataScience/DataQuest/Step6_MachineLearning/4_LinearRegressionForMachineLearning/project1/PredictingHouseSalePrices.ipynb), with the hyperlink modified.\n",
    "\n",
    "The output of the [previous step](#3.-Predict-and-evaluate) will be different each time it is run. This is because extra trees regressor is a method with intrinsic randomness. Intuitively, this is not good for making consistent predictions. I used it here because it was much faster than the recursive feature elimination (the only other method that I tried using).\n",
    "\n",
    "### 5.1.3. Activation function\n",
    "\n",
    "The rectified linear unit (ReLU) activation function was used in the neural network. This has two disadvantages. \n",
    "\n",
    "First, [ReLU turns negative values to 0](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions). This meant that numerical features could not be standardised which would have allowed variance to be taken into account in the network. This might be more favourable than min-max scaling.\n",
    "\n",
    "Second, ReLU is known to end up with dead neurons in some cases - once it is dead, it only outputs 0.\n",
    "\n",
    "An alternative would be \"leaky ReLU\" which keeps negative input negative, but it is not supported by scikit-learn library at the moment. Unfortunately, tweaking existing library or writing neural networks algorithm is currently beyond the scope of this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
