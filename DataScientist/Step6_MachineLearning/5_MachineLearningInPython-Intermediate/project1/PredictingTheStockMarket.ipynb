{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/m/65/guided-project%3A-predicting-the-stock-market\n",
    "\n",
    "In this project, I will predict each day's closing stock price for [S&P 500 Index](https://en.wikipedia.org/wiki/S%26P_500_Index) based on the past record. I will use neural networks to train the model with records from `1950-2012`, and make predictions for `2013-2015`.\n",
    "\n",
    "The dataset has been prepared by DataQuest, who describes it like the following.\n",
    "___\n",
    "Each row in the file contains a daily record of the price of the S&P500 Index from `1950` to `2015`. The dataset is stored in `sphist.csv`.\n",
    "\n",
    "The columns of the dataset are:\n",
    "\n",
    "*   `Date` \\-\\- The date of the record.\n",
    "*   `Open` \\-\\- The opening price of the day (when trading starts).\n",
    "*   `High` \\-\\- The highest trade price during the day.\n",
    "*   `Low` \\-\\- The lowest trade price during the day.\n",
    "*   `Close` \\-\\- The closing price for the day (when trading is finished).\n",
    "*   `Volume` \\-\\- The number of shares traded.\n",
    "*   `Adj Close` \\-\\- The daily closing price, adjusted retroactively to include any corporate actions. Read more [here](http://www.investopedia.com/terms/a/adjusted_closing_price.asp).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16590 entries, 0 to 16589\n",
      "Data columns (total 7 columns):\n",
      "Date         16590 non-null object\n",
      "Open         16590 non-null float64\n",
      "High         16590 non-null float64\n",
      "Low          16590 non-null float64\n",
      "Close        16590 non-null float64\n",
      "Volume       16590 non-null float64\n",
      "Adj Close    16590 non-null float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 907.3+ KB\n",
      "None\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-07</td>\n",
       "      <td>2090.419922</td>\n",
       "      <td>2090.419922</td>\n",
       "      <td>2066.780029</td>\n",
       "      <td>2077.070068</td>\n",
       "      <td>4.043820e+09</td>\n",
       "      <td>2077.070068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-04</td>\n",
       "      <td>2051.239990</td>\n",
       "      <td>2093.840088</td>\n",
       "      <td>2051.239990</td>\n",
       "      <td>2091.689941</td>\n",
       "      <td>4.214910e+09</td>\n",
       "      <td>2091.689941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-03</td>\n",
       "      <td>2080.709961</td>\n",
       "      <td>2085.000000</td>\n",
       "      <td>2042.349976</td>\n",
       "      <td>2049.620117</td>\n",
       "      <td>4.306490e+09</td>\n",
       "      <td>2049.620117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-02</td>\n",
       "      <td>2101.709961</td>\n",
       "      <td>2104.270020</td>\n",
       "      <td>2077.110107</td>\n",
       "      <td>2079.510010</td>\n",
       "      <td>3.950640e+09</td>\n",
       "      <td>2079.510010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>2082.929932</td>\n",
       "      <td>2103.370117</td>\n",
       "      <td>2082.929932</td>\n",
       "      <td>2102.629883</td>\n",
       "      <td>3.712120e+09</td>\n",
       "      <td>2102.629883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2015-12-07  2090.419922  2090.419922  2066.780029  2077.070068   \n",
       "1  2015-12-04  2051.239990  2093.840088  2051.239990  2091.689941   \n",
       "2  2015-12-03  2080.709961  2085.000000  2042.349976  2049.620117   \n",
       "3  2015-12-02  2101.709961  2104.270020  2077.110107  2079.510010   \n",
       "4  2015-12-01  2082.929932  2103.370117  2082.929932  2102.629883   \n",
       "\n",
       "         Volume    Adj Close  \n",
       "0  4.043820e+09  2077.070068  \n",
       "1  4.214910e+09  2091.689941  \n",
       "2  4.306490e+09  2049.620117  \n",
       "3  3.950640e+09  2079.510010  \n",
       "4  3.712120e+09  2102.629883  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing data point per feature:\n",
      "Date         0\n",
      "Open         0\n",
      "High         0\n",
      "Low          0\n",
      "Close        0\n",
      "Volume       0\n",
      "Adj Close    0\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from time import time\n",
    "\n",
    "# from sklearn.feature_selection import RFE\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"sphist.csv\")\n",
    "\n",
    "# Get names of original columns\n",
    "original_cols = df.columns\n",
    "\n",
    "# Set target column (closing price of the day)\n",
    "target_col = \"Close\"\n",
    "\n",
    "# Dataset summary\n",
    "print(df.info())\n",
    "print()\n",
    "\n",
    "# Display first 5 rows\n",
    "display(df.head(5))\n",
    "\n",
    "# Check for missing data\n",
    "print(\"Number of missing data point per feature:\")\n",
    "print(df.isna().sum())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First date in date: 1950-01-03 00:00:00\n",
      "First date to include in training data 1951-01-03 00:00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Holidays in previous month</th>\n",
       "      <th>Close: Past 5 days mean</th>\n",
       "      <th>Close: Past 5 days SD</th>\n",
       "      <th>Close: Past 30 days SD</th>\n",
       "      <th>Close means ratio: past 5 days / past 30 days</th>\n",
       "      <th>Close SDs ratio: past 5 days / past 30 days</th>\n",
       "      <th>Close means ratio: past 5 days / past 365 days</th>\n",
       "      <th>Close SDs ratio: past 5 days / past 365 days</th>\n",
       "      <th>Volume: Past 5 days mean</th>\n",
       "      <th>Volume: Past 365 days mean</th>\n",
       "      <th>Volume: Past 5 days SD</th>\n",
       "      <th>Volume: Past 30 days SD</th>\n",
       "      <th>Volume means ratio: past 5 days / past 30 days</th>\n",
       "      <th>Volume SDs ratio: past 5 days / past 30 days</th>\n",
       "      <th>Volume means ratio: past 5 days / past 365 days</th>\n",
       "      <th>Volume SDs ratio: past 5 days / past 365 days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.803896</td>\n",
       "      <td>0.374452</td>\n",
       "      <td>0.023061</td>\n",
       "      <td>0.416674</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.189881</td>\n",
       "      <td>0.047575</td>\n",
       "      <td>0.112902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002536</td>\n",
       "      <td>0.825820</td>\n",
       "      <td>0.262149</td>\n",
       "      <td>0.024406</td>\n",
       "      <td>0.317158</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000523</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.528329</td>\n",
       "      <td>0.124929</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>0.078290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>0.836977</td>\n",
       "      <td>0.224613</td>\n",
       "      <td>0.025173</td>\n",
       "      <td>0.289886</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.535468</td>\n",
       "      <td>0.080559</td>\n",
       "      <td>0.051535</td>\n",
       "      <td>0.054757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.848501</td>\n",
       "      <td>0.190383</td>\n",
       "      <td>0.026010</td>\n",
       "      <td>0.263050</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.498266</td>\n",
       "      <td>0.140983</td>\n",
       "      <td>0.048566</td>\n",
       "      <td>0.084911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.862964</td>\n",
       "      <td>0.137202</td>\n",
       "      <td>0.026949</td>\n",
       "      <td>0.203936</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.502812</td>\n",
       "      <td>0.187030</td>\n",
       "      <td>0.049640</td>\n",
       "      <td>0.111325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Close  Year  Month       Day  Holidays in previous month  \\\n",
       "250  0.000000   0.0    0.0  0.066667                         1.0   \n",
       "251  0.000085   0.0    0.0  0.100000                         1.0   \n",
       "252  0.000085   0.0    0.0  0.133333                         1.0   \n",
       "253  0.000147   0.0    0.0  0.233333                         1.0   \n",
       "254  0.000204   0.0    0.0  0.266667                         1.0   \n",
       "\n",
       "     Close: Past 5 days mean  Close: Past 5 days SD  Close: Past 30 days SD  \\\n",
       "250                 0.000000               0.002976                0.002239   \n",
       "251                 0.000075               0.002247                0.002536   \n",
       "252                 0.000120               0.002058                0.002793   \n",
       "253                 0.000169               0.001871                0.003083   \n",
       "254                 0.000224               0.001430                0.003388   \n",
       "\n",
       "     Close means ratio: past 5 days / past 30 days  \\\n",
       "250                                       0.803896   \n",
       "251                                       0.825820   \n",
       "252                                       0.836977   \n",
       "253                                       0.848501   \n",
       "254                                       0.862964   \n",
       "\n",
       "     Close SDs ratio: past 5 days / past 30 days  \\\n",
       "250                                     0.374452   \n",
       "251                                     0.262149   \n",
       "252                                     0.224613   \n",
       "253                                     0.190383   \n",
       "254                                     0.137202   \n",
       "\n",
       "     Close means ratio: past 5 days / past 365 days  \\\n",
       "250                                        0.023061   \n",
       "251                                        0.024406   \n",
       "252                                        0.025173   \n",
       "253                                        0.026010   \n",
       "254                                        0.026949   \n",
       "\n",
       "     Close SDs ratio: past 5 days / past 365 days  Volume: Past 5 days mean  \\\n",
       "250                                      0.416674                  0.000264   \n",
       "251                                      0.317158                  0.000277   \n",
       "252                                      0.289886                  0.000286   \n",
       "253                                      0.263050                  0.000271   \n",
       "254                                      0.203936                  0.000278   \n",
       "\n",
       "     Volume: Past 365 days mean  Volume: Past 5 days SD  \\\n",
       "250                    0.000517                0.000133   \n",
       "251                    0.000523                0.000093   \n",
       "252                    0.000530                0.000065   \n",
       "253                    0.000534                0.000101   \n",
       "254                    0.000543                0.000135   \n",
       "\n",
       "     Volume: Past 30 days SD  Volume means ratio: past 5 days / past 30 days  \\\n",
       "250                 0.000286                                        0.510870   \n",
       "251                 0.000288                                        0.528329   \n",
       "252                 0.000290                                        0.535468   \n",
       "253                 0.000283                                        0.498266   \n",
       "254                 0.000295                                        0.502812   \n",
       "\n",
       "     Volume SDs ratio: past 5 days / past 30 days  \\\n",
       "250                                      0.189881   \n",
       "251                                      0.124929   \n",
       "252                                      0.080559   \n",
       "253                                      0.140983   \n",
       "254                                      0.187030   \n",
       "\n",
       "     Volume means ratio: past 5 days / past 365 days  \\\n",
       "250                                         0.047575   \n",
       "251                                         0.050075   \n",
       "252                                         0.051535   \n",
       "253                                         0.048566   \n",
       "254                                         0.049640   \n",
       "\n",
       "     Volume SDs ratio: past 5 days / past 365 days  \n",
       "250                                       0.112902  \n",
       "251                                       0.078290  \n",
       "252                                       0.054757  \n",
       "253                                       0.084911  \n",
       "254                                       0.111325  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_correlated_features(df, corr_thres, target_col):\n",
    "    \"\"\"\n",
    "    The code is from https://bit.ly/2J4WkIw\n",
    "    \n",
    "    df: Data frame\n",
    "    corr_thres: Upper limit of correlation between features.\n",
    "                If two features are correlated above corr_thes,\n",
    "                one of them will be removed.\n",
    "    target_col: Target column which will be excluded from\n",
    "                correlation coefficient calculation\n",
    "    \n",
    "    Return data frame after removing features\n",
    "    which are correlated together beyond corr_thres\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Identify Highly Correlated Features\n",
    "    # Create correlation matrix with feature columns\n",
    "    corr_matrix = df.drop(target_col, axis=1).corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > corr_thres)]\n",
    "    \n",
    "    ## Drop Marked Features\n",
    "    # Drop features\n",
    "    df = df.drop(df[to_drop].columns, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_scaling(df, mode=\"minmax\"):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    mode: Method of scaling.\n",
    "        \"standardise\" converts values to standard scores.\n",
    "        \"minmax\" puts all values into [0, 1] range.\n",
    "    \n",
    "    Return df after replacing values in numerical columns\n",
    "    with scaled values.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        series = df[col]\n",
    "        if mode == \"standardise\":    \n",
    "            df[col] = (series - np.mean(series)) / np.std(series)\n",
    "        elif mode == \"minmax\":\n",
    "            df[col] = (series - np.min(series)) / (np.max(series) - np.min(series))\n",
    "        \n",
    "    return df\n",
    "\n",
    "def feature_aggregation(df, max_offset, first_ind_to_include):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    first_ind_to_include: Index for the first row to include in\n",
    "                          training and testing sets\n",
    "    max_offset: Training and testing sets will exclude data\n",
    "                from the first date in data up to max_offset days.\n",
    "    \n",
    "    Take rows to be included in training and testing sets.\n",
    "    Add aggregated features to them.\n",
    "    Return df.\n",
    "    \"\"\"\n",
    "\n",
    "    # For all rows to be included in training and testing\n",
    "    for ind, row in df.loc[first_ind_to_include:].iterrows():\n",
    "\n",
    "        # Add new columns for year, month, day of week and\n",
    "        # number of holidays in previous month\n",
    "        ymd = row[\"Date\"]\n",
    "        \n",
    "        df.loc[ind, \"Year\"] = ymd.year\n",
    "        df.loc[ind, \"Month\"] = ymd.month\n",
    "        df.loc[ind, \"Day\"] = ymd.day\n",
    "        df.loc[ind, \"Holidays in previous month\"] = (ymd - pd.DateOffset(months=1)).days_in_month\n",
    "\n",
    "\n",
    "\n",
    "        # For Close (closing price) and Volume columns\n",
    "        for col in [\"Close\", \"Volume\"]:\n",
    "\n",
    "            # Get values for the past (1) 5 and 30 trading days\n",
    "            # and (2) max_offset days including holidays\n",
    "            ind_5 = slice(ind - 5, ind)\n",
    "            ind_30 = slice(ind - 30, ind)\n",
    "            ind_max = slice(df[df[\"Date\"] <= ymd - pd.DateOffset(days=max_offset)].index[0], ind)\n",
    "\n",
    "            val_5 = df.loc[ind_5, \"{}\".format(col)]\n",
    "            val_30 = df.loc[ind_30, \"{}\".format(col)]\n",
    "            val_max = df.loc[ind_max, \"{}\".format(col)]\n",
    "\n",
    "            # Add new columns of ...\n",
    "            ## Value mean\n",
    "            val_5_mean = np.mean(val_5)\n",
    "            val_30_mean = np.mean(val_30)\n",
    "            val_max_mean = np.mean(val_max)\n",
    "\n",
    "            df.loc[ind, \"{}: Past 5 days mean\".format(col)] = val_5_mean\n",
    "            df.loc[ind, \"{}: Past 30 days mean\".format(col)] = val_30_mean\n",
    "            df.loc[ind, \"{}: Past {} days mean\".format(col, max_offset)] = val_max_mean\n",
    "\n",
    "            ## Value SD\n",
    "            val_5_sd = np.std(val_5)\n",
    "            val_30_sd = np.std(val_30)\n",
    "            val_max_sd = np.std(val_max)\n",
    "\n",
    "            df.loc[ind, \"{}: Past 5 days SD\".format(col)] = val_5_sd\n",
    "            df.loc[ind, \"{}: Past 30 days SD\".format(col)] = val_30_sd\n",
    "            df.loc[ind, \"{}: Past {} days SD\".format(col, max_offset)] = val_max_sd\n",
    "\n",
    "            ## Ratios between statistics from different periods in the past\n",
    "            ### 5 days vs 30 days\n",
    "            df.loc[ind, \"{} means ratio: past 5 days / past 30 days\".format(col)] = val_5_mean / val_30_mean\n",
    "            df.loc[ind, \"{} SDs ratio: past 5 days / past 30 days\".format(col)] = val_5_sd / val_30_sd\n",
    "\n",
    "            ### 5 days and max_offset days\n",
    "            df.loc[ind, \"{} means ratio: past 5 days / past {} days\".format(col, max_offset)] = val_5_mean / val_max_mean\n",
    "            df.loc[ind, \"{} SDs ratio: past 5 days / past {} days\".format(col, max_offset)] = val_5_sd / val_max_sd\n",
    "    \n",
    "    return df\n",
    "\n",
    "def drop_excluded(df, first_ind_to_include, original_cols, target_col):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    first_ind_to_include: Index of first row to include\n",
    "    original_cols: Original columns which will be removed to avoid data leakage\n",
    "                   (I am trying to predict each day's value based on info\n",
    "                   from PREVIOUS DAYS. Therefore, any info from each day, which\n",
    "                   the original columns contain, should not be input into\n",
    "                   the learning algorighm.)\n",
    "    target_col: Target column which contains values to be predicted\n",
    "    \"\"\"\n",
    "    # Remove rows to be excluded from training and testing sets\n",
    "    df = df.loc[first_ind_to_include:]\n",
    "\n",
    "    # Remove original columns to avoid data leakage\n",
    "    df = df.drop(original_cols.drop(target_col), axis=1)\n",
    "\n",
    "    return df\n",
    "    \n",
    "def feature_processing(df, max_offset, original_cols, target_col, corr_thres):\n",
    "    \"\"\"\n",
    "    df: pandas data frame\n",
    "    max_offset: Training and testing sets will exclude the data\n",
    "                    up to [first date in data + max_offset days]\n",
    "    original_cols: Columns from original dataset.\n",
    "    target_col: Target column which contains values to be predicted\n",
    "    corr_thres: Upper limit of correlation between features.\n",
    "                If two features are correlated above corr_thes,\n",
    "                one of them will be removed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get first date to include (= max_offset days after the first date in data)\n",
    "    # in training and testing sets\n",
    "    first_date_in_data = df.iloc[0][\"Date\"]\n",
    "    first_date_to_include = first_date_in_data + pd.DateOffset(days=max_offset)\n",
    "    first_ind_to_include = df.index[(df[\"Date\"] >= first_date_to_include)][0]\n",
    "\n",
    "    print(\"First date in date:\", first_date_in_data)\n",
    "    print(\"First date to include in training data\", first_date_to_include)\n",
    "\n",
    "    # Create aggregated features for training and testing\n",
    "    df = feature_aggregation(df, max_offset, first_ind_to_include)\n",
    "    \n",
    "    # Remove rows and columns to be excluded from training and testing sets\n",
    "    df = drop_excluded(df, first_ind_to_include, original_cols, target_col)\n",
    "\n",
    "    # Scale numerical features\n",
    "    df = feature_scaling(df, mode=\"minmax\")\n",
    "\n",
    "    # Drop features which are highly correlated with one another\n",
    "    df = drop_correlated_features(df, corr_thres, target_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Format Date column\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "\n",
    "# Sort data frame by Date column and reset index\n",
    "df = df.sort_values(by=\"Date\")\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Feature processing - create new columns based on existing ones\n",
    "df = feature_processing(df, 365, original_cols, target_col, corr_thres=0.95)\n",
    "\n",
    "# Display first 5 rows of data frame\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Predict and evaluate\n",
    "\n",
    "The following comment and code have been taken from the section \"6. Predict and evaluate\" of [my other machine learning project](https://github.com/gknam/dataquest_projects/blob/master/DataScientist/Step6_MachineLearning/4_LinearRegressionForMachineLearning/project1/PredictingHouseSalePrices.ipynb) and slightly modified - feature selection is made 15 times in each fold and mean absolute error is used to evaluate prediction accuracy.\n",
    "\n",
    "___\n",
    "\n",
    "K-fold cross validation will be carried out where K will range from 2 to and including 10.\n",
    "\n",
    "In each fold, feature selection will be done based on each feature's importance which will be evaluated using [ExtraTreesRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor).\n",
    "\n",
    "Feature selection will be done 15 times per fold. First selection will include the most important feature. The second selection will be the first selection plus the next most important feature. The same will be done for up to a selection of 20 most important features.\n",
    "\n",
    "With each set of selected features, predictions will be made using [linear regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and then evaluated using [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error) (MAE). The resulting MAEs (n=15) will be averaged within each fold.\n",
    "\n",
    "Finally, the best fold will be selected and its MAE will be reported together with the names and numbers of selected features.\n",
    "\n",
    "**Note**: Feature selection is done ***during*** cross validation rather than beforehand. [This will prevent bias which can be created from using feature sets selected from the *whole* dataset to make prediction on *subsets* of data. By doing this, cross validation assesses the **model fitting process** rather than the model itself](https://stats.stackexchange.com/a/27751).\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 953.01 seconds\n",
      "\n",
      "K-fold cross validation was tried with K ranging from 2 to and including 9.\n",
      "\n",
      "Feature selection was made using ExtraTreesRegressor.\n",
      "Different selection sizes were tried with\n",
      "smallest set including 1 features\n",
      "and maximum one including 15 features.\n",
      "\n",
      "Best prediction was made with MAE 0.0029679271383957232 in\n",
      "(1) 5-fold cross-validation\n",
      "(2) with 2 best features selected\n",
      "\n",
      "The best feature sets selected in each fold were\n",
      "\n",
      "'Close: Past 5 days mean', 'Volume: Past 5 days mean' and\n",
      "\n",
      "'Close: Past 5 days mean', 'Volume: Past 365 days mean'\n",
      "\n",
      "\n",
      "(Each feature set lists features in order of importance)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2-fold CV</th>\n",
       "      <th>3-fold CV</th>\n",
       "      <th>4-fold CV</th>\n",
       "      <th>5-fold CV</th>\n",
       "      <th>6-fold CV</th>\n",
       "      <th>7-fold CV</th>\n",
       "      <th>8-fold CV</th>\n",
       "      <th>9-fold CV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1 feature</th>\n",
       "      <td>0.021631</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.008106</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.006794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 feature</th>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.003781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 feature</th>\n",
       "      <td>0.004044</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.003418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 feature</th>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.005992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 feature</th>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>0.004026</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.003586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 feature</th>\n",
       "      <td>0.005068</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.003297</td>\n",
       "      <td>0.003359</td>\n",
       "      <td>0.003765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 feature</th>\n",
       "      <td>0.005353</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.003563</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.003341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8 feature</th>\n",
       "      <td>0.004495</td>\n",
       "      <td>0.003829</td>\n",
       "      <td>0.003789</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.003517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 feature</th>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.003716</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.003521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 feature</th>\n",
       "      <td>0.004690</td>\n",
       "      <td>0.004075</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.004001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11 feature</th>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.004875</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.003869</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.003638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12 feature</th>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.004334</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.003727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13 feature</th>\n",
       "      <td>0.005394</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.004766</td>\n",
       "      <td>0.004092</td>\n",
       "      <td>0.003813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14 feature</th>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.004392</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.004683</td>\n",
       "      <td>0.004073</td>\n",
       "      <td>0.004342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15 feature</th>\n",
       "      <td>0.005420</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.004373</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.004424</td>\n",
       "      <td>0.005734</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>0.004479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            2-fold CV  3-fold CV  4-fold CV  5-fold CV  6-fold CV  7-fold CV  \\\n",
       "1 feature    0.021631   0.003386   0.003714   0.009470   0.008372   0.008106   \n",
       "2 feature    0.004757   0.004263   0.004921   0.002968   0.004149   0.004371   \n",
       "3 feature    0.004044   0.003632   0.009702   0.003623   0.007434   0.003700   \n",
       "4 feature    0.004300   0.003621   0.003326   0.003734   0.003126   0.003471   \n",
       "5 feature    0.004046   0.003864   0.004026   0.003514   0.003400   0.003583   \n",
       "6 feature    0.005068   0.003551   0.003464   0.004172   0.003110   0.003297   \n",
       "7 feature    0.005353   0.004360   0.003563   0.003587   0.003716   0.003671   \n",
       "8 feature    0.004495   0.003829   0.003789   0.003938   0.003780   0.003817   \n",
       "9 feature    0.004458   0.004047   0.003846   0.003848   0.003716   0.004030   \n",
       "10 feature   0.004690   0.004075   0.004300   0.003978   0.004058   0.004235   \n",
       "11 feature   0.005481   0.004875   0.004099   0.004140   0.003869   0.003816   \n",
       "12 feature   0.005557   0.004212   0.004334   0.004004   0.004095   0.003952   \n",
       "13 feature   0.005394   0.004385   0.004027   0.003878   0.003960   0.004766   \n",
       "14 feature   0.004680   0.004655   0.004641   0.004392   0.004548   0.004683   \n",
       "15 feature   0.005420   0.005029   0.004373   0.004522   0.004424   0.005734   \n",
       "\n",
       "            8-fold CV  9-fold CV  \n",
       "1 feature    0.003163   0.006794  \n",
       "2 feature    0.003769   0.003781  \n",
       "3 feature    0.003378   0.003418  \n",
       "4 feature    0.003200   0.005992  \n",
       "5 feature    0.003218   0.003586  \n",
       "6 feature    0.003359   0.003765  \n",
       "7 feature    0.003497   0.003341  \n",
       "8 feature    0.003578   0.003517  \n",
       "9 feature    0.003588   0.003521  \n",
       "10 feature   0.003603   0.004001  \n",
       "11 feature   0.003843   0.003638  \n",
       "12 feature   0.003841   0.003727  \n",
       "13 feature   0.004092   0.003813  \n",
       "14 feature   0.004073   0.004342  \n",
       "15 feature   0.004409   0.004479  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feature_selection(selector, features, target, top_features_number):\n",
    "\n",
    "    selector.fit(features, target)\n",
    "    \n",
    "    feature_cols_series = pd.Series(feature_cols, \\\n",
    "                                    name=\"Feature_cols\")\n",
    "    \n",
    "    if type(selector) == ExtraTreesRegressor:\n",
    "        feature_significance = selector.feature_importances_\n",
    "        feature_significance_label = \"Feature_importance_score\"\n",
    "        ascending = False\n",
    "\n",
    "    elif type(selector) == RFE:\n",
    "        feature_significance = selector.ranking_\n",
    "        feature_significance_label = \"Feature_importance_ranking\"\n",
    "        ascending = True\n",
    "        \n",
    "    \n",
    "    feature_significance_series = pd.Series(feature_significance, \\\n",
    "                                          name=feature_significance_label)\n",
    "\n",
    "    feature_cols_top = pd.concat([feature_cols_series, feature_significance_series], axis=1)\\\n",
    "            .sort_values(by=feature_significance_label, ascending=ascending)\\\n",
    "            .iloc[:top_features_number, 0].values\n",
    "            \n",
    "    return feature_cols_top\n",
    "\n",
    "def train_and_test(fs_train, fs_test, t_train, t_test, model):\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(fs_train, t_train)\n",
    "    \n",
    "    # Predict target using test dataset\n",
    "    p_test = model.predict(fs_test)\n",
    "    \n",
    "    # Get MAE (mean absolute error)\n",
    "    mae = mean_absolute_error(t_test, p_test)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "\n",
    "# Get features and target\n",
    "feature_cols = df.columns.drop(target_col)\n",
    "\n",
    "features = df[feature_cols]\n",
    "target = df[target_col]\n",
    "\n",
    "# Calculate number of neurons in hidden layer\n",
    "n_input = (features.shape[1] + 1)\n",
    "n_output = 1\n",
    "n_sample = features.shape[0]\n",
    "alpha = 2\n",
    "n_hidden = int(n_sample / (alpha * (n_input + n_output)))\n",
    "\n",
    "# Model to use for prediction\n",
    "model = MLPRegressor(hidden_layer_sizes=n_hidden)\n",
    "\n",
    "# Track lowest MAE\n",
    "lowest_mae = df.max().max() - df.min().min()\n",
    "\n",
    "# Track best method\n",
    "# [fold, number of selected features, \n",
    "# names of selected features, lowest MAE]\n",
    "best_method = [None, None, None, lowest_mae]\n",
    "\n",
    "\n",
    "start = time()\n",
    "\n",
    "# # initiate RFE feature selector\n",
    "# estimator = SVR(kernel=\"linear\")\n",
    "# selector = RFE(estimator, 1, step=1)\n",
    "\n",
    "# Initiate ExtraTreesRegressor feature selector\n",
    "selector = ExtraTreesRegressor()\n",
    "\n",
    "num_folds = range(2, 10)\n",
    "mae_all = {str(fold) + \"-fold CV\": [] for fold in num_folds}\n",
    "feature_set_sizes = range(1, 16)\n",
    "\n",
    "# K-fold cross validation\n",
    "for fold in num_folds:\n",
    "    \n",
    "    mae_split = []\n",
    "    kf = KFold(n_splits=fold, shuffle=True)\n",
    "\n",
    "    mae_selections = {}\n",
    "    feature_selections = {}\n",
    "    \n",
    "    kf_splits = kf.split(features)\n",
    "    \n",
    "    for (train_ind, test_ind), split in zip(kf_splits, range(fold)):\n",
    "\n",
    "        mae_selections[split] = []\n",
    "        feature_selections[split] = []\n",
    "        \n",
    "        for fss in feature_set_sizes:\n",
    "\n",
    "            # Get target\n",
    "            t_train = target.iloc[train_ind]\n",
    "            t_test = target.iloc[test_ind]\n",
    "            \n",
    "            # Get features\n",
    "            f_train = features.iloc[train_ind]\n",
    "            f_test = features.iloc[test_ind]\n",
    "            \n",
    "            # Select features\n",
    "            fs_cols = feature_selection(selector, f_train, t_train, top_features_number=fss)\n",
    "            \n",
    "            fs_train = f_train[fs_cols]\n",
    "            fs_test = f_test[fs_cols]\n",
    "            \n",
    "            # record MAE and selected features\n",
    "            mae = train_and_test(fs_train, fs_test, t_train, t_test, model)\n",
    "            mae_selections[split].append(mae)\n",
    "            feature_selections[split].append(fs_cols)\n",
    "        \n",
    "    # Get mean MAE and feature names per feature selection\n",
    "    mae_array = np.array([mae_selections[i] for i in mae_selections])\n",
    "    feature_array = np.array([feature_selections[i] for i in feature_selections])\n",
    "    for fs in feature_set_sizes:\n",
    "        mae_fs_mean = np.mean(mae_array[:, fs - 1])\n",
    "        features_fs = feature_array[:, fs - 1]\n",
    "        \n",
    "        if mae_fs_mean < lowest_mae:\n",
    "            lowest_mae = mae_fs_mean\n",
    "            best_method = [fold, fs, features_fs, lowest_mae]\n",
    "        \n",
    "        mae_all[str(fold) + \"-fold CV\"].append(mae_fs_mean)\n",
    "    \n",
    "end = time()\n",
    "\n",
    "# Get unique feature selections in best_method\n",
    "# (source https://stackoverflow.com/a/3724558)\n",
    "features_fs_all = [list(x) for x in set(tuple(x) for x in best_method[2])]\n",
    "\n",
    "# Get selector name\n",
    "selector_type_str = str((type(selector)))\n",
    "selector_name = re.sub(\".*\\.|'.*$\", \"\", selector_type_str)\n",
    "\n",
    "print(\"Duration: \" + (\"{:.2f}\").format(end - start) + \" seconds\")\n",
    "print()\n",
    "\n",
    "print(\"K-fold cross validation was tried with K ranging from {} to and including {}.\"\\\n",
    "     .format(min(num_folds), max(num_folds)))\n",
    "print()\n",
    "print(\"Feature selection was made using {}.\".format(selector_name))\n",
    "print(\"Different selection sizes were tried with\")\n",
    "print(\"smallest set including {} features\".format(min(feature_set_sizes)))\n",
    "print(\"and maximum one including {} features.\".format(max(feature_set_sizes)))\n",
    "print()\n",
    "\n",
    "print(\"Best prediction was made with MAE {} in\".format(best_method[3]))\n",
    "print(\"(1) {}-fold cross-validation\".format(best_method[0]))\n",
    "print(\"(2) with {} best features selected\".format(best_method[1]))\n",
    "print()\n",
    "print(\"The best feature sets selected in each fold were\")\n",
    "print()\n",
    "for ind, val in enumerate(features_fs_all):\n",
    "    val = re.sub(\"\\[|\\]\", \"\", str(val))\n",
    "    to_print = str(val) + \" and\" if ind < len(features_fs_all) - 1 \\\n",
    "                              else str(val)\n",
    "    \n",
    "    print(to_print)\n",
    "    print()\n",
    "print()\n",
    "print(\"(Each feature set lists features in order of importance)\")\n",
    "    \n",
    "mae_all_df = pd.DataFrame(data=mae_all, index=[str(i) + \" feature\" for i in feature_set_sizes])\n",
    "mae_all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Closing remarks\n",
    "\n",
    "## 4.1. Limitation\n",
    "\n",
    "### 4.1.2. Feature selection method\n",
    "\n",
    "Again, the comment has been taken from section \"7.1.2. Feature selection method\" of [my other machine learning project](https://github.com/gknam/dataquest_projects/blob/master/DataScientist/Step6_MachineLearning/4_LinearRegressionForMachineLearning/project1/PredictingHouseSalePrices.ipynb), with the hyperlink modified.\n",
    "\n",
    "The output of the [previous step](#3.-Predict-and-evaluate) will be different each time it is run. This is because extra trees regressor is a method with intrinsic randomness. Intuitively, this is not good for making consistent predictions. I used it here because it was much faster than the recursive feature elimination (the only other method that I tried using)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
